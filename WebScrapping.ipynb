{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d337a006",
   "metadata": {},
   "source": [
    "# FlipKart Web Scraping\n",
    "#### Using Python\n",
    "\n",
    "* Web Scraping is also known as web harvesting, or web data extraction is a process of extracting data from websites.\n",
    "* This is a project which uses python to extract data from **FlipKart**.\n",
    "\n",
    "Every code chunk is explained below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0583b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import smtplib\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df88c986",
   "metadata": {},
   "source": [
    "### 2. Step 2 is to set the URL variable to the URL from which we want to extract the data\n",
    "* **Requests** is the package which provides a method **GET** to get the HTML page of the link\n",
    "* In the **try-catch** block send a request to the link to get a copy of the HTML page \n",
    "* It is also a good practice to add hearders to the GET method\n",
    "* The result is not formated\n",
    "* To convert it to a formatted HTML code we use **BeautifulSoup**\n",
    "* Now soup2 has the prettified version of the HTML code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df158891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to website\n",
    "URL = \"https://www.flipkart.com/search?q=shoes&sid=osp%2Ccil%2Ce1f&as=on&as-show=on&otracker=AS_QueryStore_OrganicAutoSuggest_2_5_na_na_na&otracker1=AS_QueryStore_OrganicAutoSuggest_2_5_na_na_na&as-pos=2&as-type=RECENT&suggestionId=shoes%7CCasual+Shoes&requestId=c04f4b6f-d0dd-4f01-8f17-27adb9693103&as-searchtext=shoes\"\n",
    "\n",
    "# system info link to find this --> \"https://httpbin.org/get\"\n",
    "header = {\"User-Agent\": \"fill this with your system info\"}\n",
    "\n",
    "try:\n",
    "    page = requests.get(URL, headers = header)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "soup1 = BeautifulSoup(page.content, \"html.parser\") #lxml for broken html pages\n",
    "soup2 = BeautifulSoup(soup1.prettify(),\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30defe8e",
   "metadata": {},
   "source": [
    "### 3. Extract the required Info from the HTML code using tags and class names\n",
    "* Different attributes like product name, brand name or the discount can be extracted\n",
    "* But first, we need to get the div which has all this info\n",
    "* In order to find all the different items we need to use find all and it returns a list of the items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d79c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = soup2.find_all('div', {'class': '_1xHGtK _373qXS'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be4eef9",
   "metadata": {},
   "source": [
    "### 4. Open a CSV file in which all the data will be stored and add the headers i.e. the name of the columns which will be entered in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a csv file and add the headers\n",
    "header = ['brand_name','product_name','original_price', 'discounted_price', 'discount','link','date']\n",
    "with open('FlipKartWebScraper.csv', 'w', newline='', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e9116",
   "metadata": {},
   "source": [
    "### 5. Use a for loop to iterate all the items and the required details of the product\n",
    "* To get a particular item from the list find is the method to use\n",
    "* It takes the name of the tag and the class name\n",
    "* It returns the whole tag to get the text use .text after the find method\n",
    "* The data will have extra white spaces to remove that use the strip method to get the required data\n",
    "* Numeric data should be converted into integers before storing\n",
    "* Colle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2add2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in items:\n",
    "    brand_name = item.find('div',{'class': '_2WkVRV'}).text\n",
    "    link = item.find('a',{'class': 'IRpwTa'})['href']\n",
    "    product_name = item.find('a',{'class': 'IRpwTa'})['title']\n",
    "    discounted_price = item.find('div',{'class': '_30jeq3'}).text\n",
    "    original_price = item.find('div',{'class': '_3I9_wc'}).text\n",
    "    discount = item.find('div',{'class': '_3Ay6Sb'}).text\n",
    "    \n",
    "    # clean the data\n",
    "    brand_name = product_name.strip()\n",
    "    product_name = product_name.strip()\n",
    "    today = datetime.date.today()\n",
    "    original_price = int(original_price.strip()[1:].replace(',',''))\n",
    "    discounted_price = int(discounted_price.strip()[1:].replace(',',''))\n",
    "    discount = int(discount.strip()[0:2])\n",
    "    link = link.strip()\n",
    "   \n",
    "    # append the data to the csv file\n",
    "    data = [brand_name, product_name, original_price, discounted_price, discount, link, today]\n",
    "    with open('FlipKartWebScraper.csv', 'a+', newline='', encoding='UTF8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f5111",
   "metadata": {},
   "source": [
    "### To sum it up\n",
    "* The above code will add the data one time when we run the code\n",
    "* Web Scraping can be used to keep track of a product or the price of a stock etc\n",
    "* To do this the above code must repeat itself after an interval\n",
    "* To repeat the code, use the time package as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_price():\n",
    "    URL = \"https://www.flipkart.com/search?q=shoes&sid=osp%2Ccil%2Ce1f&as=on&as-show=on&otracker=AS_QueryStore_OrganicAutoSuggest_2_5_na_na_na&otracker1=AS_QueryStore_OrganicAutoSuggest_2_5_na_na_na&as-pos=2&as-type=RECENT&suggestionId=shoes%7CCasual+Shoes&requestId=c04f4b6f-d0dd-4f01-8f17-27adb9693103&as-searchtext=shoes\"\n",
    "    header = {\"User-Agent\": \"fill this with your system info\"}\n",
    "\n",
    "    try:\n",
    "        page = requests.get(URL, headers = header)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    soup1 = BeautifulSoup(page.content, \"html.parser\") #lxml package for broken html pages\n",
    "    soup2 = BeautifulSoup(soup1.prettify(),\"html.parser\")\n",
    "    \n",
    "    items = soup2.find_all('div', {'class': '_1xHGtK _373qXS'})\n",
    "    \n",
    "    header = ['brand_name','product_name','original_price', 'discounted_price', 'discount','link','date']\n",
    "    with open('FlipKartWebScraper.csv', 'w', newline='', encoding='UTF8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        \n",
    "    for item in items:\n",
    "        brand_name = item.find('div',{'class': '_2WkVRV'}).text\n",
    "        link = item.find('a',{'class': 'IRpwTa'})['href']\n",
    "        product_name = item.find('a',{'class': 'IRpwTa'})['title']\n",
    "        discounted_price = item.find('div',{'class': '_30jeq3'}).text\n",
    "        original_price = item.find('div',{'class': '_3I9_wc'}).text\n",
    "        discount = item.find('div',{'class': '_3Ay6Sb'}).text\n",
    "\n",
    "        # clean the data\n",
    "        brand_name = product_name.strip()\n",
    "        product_name = product_name.strip()\n",
    "        today = datetime.date.today()\n",
    "        original_price = int(original_price.strip()[1:].replace(',',''))\n",
    "        discounted_price = int(discounted_price.strip()[1:].replace(',',''))\n",
    "        discount = int(discount.strip()[0:2])\n",
    "        link = link.strip()\n",
    "\n",
    "        # append the data to the csv file\n",
    "        data = [brand_name, product_name, original_price, discounted_price, discount, link, today]\n",
    "        with open('FlipKartWebScraper.csv', 'a+', newline='', encoding='UTF8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d609027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check price every day\n",
    "while(True):\n",
    "    import time\n",
    "    check_price()\n",
    "    time.sleep(86400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dffb54",
   "metadata": {},
   "source": [
    "> #### This is for any website change the url and attribute names and values accordingly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
